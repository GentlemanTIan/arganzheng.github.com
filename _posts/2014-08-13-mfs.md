---
title: 静态资源服务器迁移MFS方案
layout: post
---


背景
----

目前的静态资源以普通Linux文件系统（Ext3）的方式存放在05机器上，主要是图片和新闻（以及新闻附带的图片）。每半个小时更新一次。通过rsync + cron 脚本定时同步到01。01只是备份，不提供服务。随着静态资源的增加，静态服务器的压力比较大，而且读写服务是单点，虽然有冗余容灾，但是不能同时提供服务，不具有线性拓展性。一般来说海量的文件系统都需要使用分布式文件系统。具体参见[海量图片存储思考](http://blog.arganzheng.me/posts/finding-a-needle-in-haystack.html)。


关于MFS
-------

MFS是Moose File System的简称，与我们熟悉的fat，ntfs，ext3一样是一种文件系统（驼鹿文件系统）。与普通的文件系统不同，MFS是一种分布式的文件系统，类似的分布式的文件系统还有NFS，Hadoop，HDFS，Lustre，GFS等。准确的来说，MFS是对Google的GFS的开源实现。MFS因其高可靠、动态扩展、灾难冗余等诸多优秀特性，当然最重要是因为MFS是开源的，被广泛的应用于海量文件分布式存贮等场景中。

MFS的特性主要有以下几点：

1. MFS是通用的文件系统，不需要修改上层应用就可以使用。MFS中的所有基本操作和普通的文件系统中的操作无任何差异，你可以直接cp，mv，rm，ls，无需特殊指令。
2. MFS支持在线扩容，体系架构的动态可扩展性极强。也就是说可以在任何时刻增加或减少MFS集群中的存储机器，而对MFS集群的整体运作部产生任何影响。
3. MFS具有高容错性，不存在单点故障。在MFS中，单一机器（准确的说是存储机器）的故障被认为是常态事件。MFS对单一机器的故障无感，前提是文件设置了多个副本。
4. MFS提供垃圾回收机制。被删除的文件不会立刻从MFS系统中消失，而是被存放在回收站中，超过了回收时间之后才会真正被删除，这样可以有效防止误操作。
5. MFS是Google File System的一个C的开源实现。MFS的理论依据是“The Google File System”这篇论文，有谷歌提供的强大理论依据。同时，MFS是开源的，有专门的开发团队进行开发维护。

除了上述的这些特性之外，MFS还提供了文件备份份数设置，文件快照Snapshot等功能，针对随机读或写和海量小文件的读写也有一定的优化。


迁移MFS方案
-----------

**步骤**

1. 搭建MFS和mount MFS（假设是/home/work/mnt/mfs/mbrowser目录） 
2. 编写同步脚本，同步 STATIC==>MFS
3. 保险起见，停止静态资源写服务（Spider和Guanxing）
4. 修改同步脚本，将同步方向变为MFS==>STATIC
5. 修改服务访问MFS目录(/home/work/mnt/mfs/mbrowser)，其中服务包括nginx，发布和重启应用。
6. 验证服务 


故障回滚方案
------------

为了防止MFS故障导致应用不可用，这里做了一些措施方便回滚。

### 1. 使用inotify机制实现准实时备份

迁移MFS之后，我们会同时将MFS文件同步回老的STATIC目录。为了保证实时性和性能，使用inotify事件监听处理机制。具体参见[如何实时同步大量小文件](http://family.baidu.com/portal/techForum/detail?articleId=699596811)。

	settings {
	    statusFile = "/tmp/lsyncd.stat",
	    logfile    = "/tmp/lsyncd.log",
	    logfacility = daemon,
	    statusIntervall = 20,
	    maxDelays    = 10,
	    maxProcesses = 4
	}

	sync {
	    default.direct,
	    source="/home/work/STATIC",
	    target="/home/work/mnt/mfs"
	}

### 2. nginx动静分离避免静态资源故障拖垮所有接口

为了避免静态资源出现问题，导致整个nginx都不可用。影响所有接口。我们把所有的静态资源请求统一转发给05的nginx处理。01和02的nginx主要负责转发动态请求给tomcat和静态请求给nginx@05。

并且为了避免卡住太久，对于静态资源响应时间设置了比较短的超时时间，并且设置了允许的最大错误次数。如果静态资源出现问题，那么当05返回的错误次数超过max_failed，01和02的nginx就会将05摘除。也就是01和02还能提供动态请求。

**TIPS** If you use only one upstream server, the max_fails & fail_timeout parameter are ignored. 所以要达到自动摘除不可用backend服务，至少需要配置两个backend server。我们下面刚好会配置这么一个。


### 3. 老的STATIC静态资源作为备份服务

经过前面的两个步骤，MFS就算出问题，顶多也就是静态资源不可用，NTM的接口层动态请求并不会受到影响。但是更进一步的，还能够让MFS也有读备份。就是搭建05的nginx备份，也是提供静态资源服务，但是这个nginx指向的是老的STATIC目录。也就是说老的静态资源服务也通过nginx提供服务了。

   	upstream static-mbrowser {
      	server hk01-hao123-mob05.hk01:10000 max_fails=3 fail_timeout=20; # MFS
      	server hk01-hao123-mob31.hk01:10000;	# old STATIC
   	}

但是这种情况下，老的STATIC目录也提供服务了。虽然有inotify作为增量同步，但是也不能保证百分百实时。所以我们希望老的STATIC目录只有当MFS出现问题的时候才做为备份提供服务。查看了一下nginx的文档，果然有这个配置项[upstream - backup](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream)：

> backup
>     marks the server as a backup server. It will be passed requests when the primary servers are unavailable.

于是只需要简单增加一个配置项：

   	upstream static-mbrowser {
      	server hk01-hao123-mob05.hk01:10000 max_fails=2 fail_timeout=30s; # MFS
      	server hk01-hao123-mob31.hk01:10000 backup;	# old STATIC
   	}

这样，假设MFS真的挂了，那么读请求通过nginx在失败2次之后，会在30s内将请求都转移到老的STATIC服务器。这样，我们把MFS故障的影响范围和影响时间都大大的缩小在可控的范围内。
当然这里有两个地方需要注意一下：

1. 老的STATIC服务器需要相对够实时，否则新的静态资源会请求不到。这个经过测试和观察，只要inotify运行着，同步是非常实时的。
2. 这里只是对读操作进行了自动切换，写操作切换需要手工修改配置项发布。

### 回滚步骤

跟切换MFS一样的步骤：

**步骤**

1. 保险起见，停止静态资源写服务（Spider和Guanxing）
2. 停止同MFS==>STATIC同步脚本，将同步方向变为MFS==>STATIC
3. 修改服务访问老的STATIC目录，其中服务包括nginx，发布和重启应用。
4. 验证服务 


最终部署图
----------

![MFS](/media/images/MFS.JPG)

